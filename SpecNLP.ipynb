{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkM8CzMF5r7x"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmxgNYBcPU1l"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Suresoft-GLaDOS/SBFL\n",
        "%cd SBFL\n",
        "%pip install -r requirements.txt\n",
        "%pip install setuptools\n",
        "!python setup.py install\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dMss9cjki1Qi"
      },
      "outputs": [],
      "source": [
        "import SBFL.sbfl.utils as sbfl_utils\n",
        "import SBFL.sbfl.base as sbfl_base\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import collections\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqZjqCgT-TwM"
      },
      "outputs": [],
      "source": [
        "PATH_GOOGLE_DRIVE = '/content/drive/MyDrive/'\n",
        "TOKENIZER         = transformers.AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "CODEBERT_BASE     = transformers.TFRobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "MAX_LEN_TOKEN     = 1024\n",
        "BATCH_SIZE        = 1\n",
        "TEST_SIZE         = 0.15\n",
        "USE_TPU           = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuLqppVORy8I"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    X: coverage data\n",
        "    y: test results\n",
        "    \"\"\"\n",
        "    X = np.array([\n",
        "        [1,0,1], # coverage of test t0\n",
        "        [1,0,1], # coverage of test t1\n",
        "        [1,1,1]  # coverage of test t2\n",
        "    ], dtype=bool)\n",
        "\n",
        "    y = np.array([\n",
        "        0, # t0: PASS\n",
        "        0, # t1: FAIL\n",
        "        0  # t2: PASS\n",
        "    ], dtype=bool)\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the suspiciousness scores\n",
        "    \"\"\"\n",
        "    sbfl = sbfl_base.SBFL(formula='Ochiai')\n",
        "    print(sbfl.fit_predict(X, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSwV4ZGmY2RW"
      },
      "outputs": [],
      "source": [
        "with open(f'{PATH_GOOGLE_DRIVE}/dataset_dict.pickle', \"rb\") as f:\n",
        "    dataset_dict = pickle.load(f)\n",
        "\n",
        "# shuffle dataset_dict\n",
        "dataset_dict = pd.DataFrame(dataset_dict).sample(frac=1).to_dict('list')\n",
        "\n",
        "\n",
        "dataset_dict_x   = {key: dataset_dict[key] for key in dataset_dict if key in ['input_ids', 'attention_mask', 'attention_spectrum']}\n",
        "dataset_dict_y   = {key: dataset_dict[key] for key in dataset_dict if key in ['start_token_idx', 'end_token_idx']}\n",
        "dataset_spectrum = {key: dataset_dict[key] for key in dataset_dict if key in ['dct_weight_error_suspicious', 'lst_line_num_error']}\n",
        "\n",
        "\n",
        "# Creating tf.data.Dataset after parse all data\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices((dataset_dict_x, dataset_dict_y)).batch(8)\n",
        "next(iter(tf_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9FPGmukYP8k"
      },
      "outputs": [],
      "source": [
        "def create_model(num_spectrum_formulas):\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN,), name = 'input_ids', dtype=tf.int32)\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN,), name = 'attention_mask', dtype=tf.int32)\n",
        "    attention_spectrum = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN, num_spectrum_formulas), name = 'attention_spectrum', dtype=tf.float32)\n",
        "\n",
        "    CODEBERT_BASE = transformers.TFRobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "    output_codebert_base = CODEBERT_BASE([input_ids, attention_mask]).last_hidden_state\n",
        "    output_concat = tf.keras.layers.Lambda(lambda x: tf.concat([x[0], x[1]], -1))([output_codebert_base, attention_spectrum])\n",
        "\n",
        "    output_concat = tf.keras.layers.Dense(1024, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(512, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(64, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(16, use_bias=False)(output_concat)\n",
        "\n",
        "\n",
        "    start_logits = tf.keras.layers.Dense(1, use_bias=False)(output_concat)\n",
        "    start_logits = tf.keras.layers.Flatten()(start_logits)\n",
        "    start_probs  = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"start_token_idx\")(start_logits)\n",
        "\n",
        "\n",
        "    end_logits = tf.keras.layers.Dense(1, use_bias=False)(output_concat)\n",
        "    end_logits = tf.keras.layers.Flatten()(end_logits)\n",
        "    end_probs  = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"end_token_idx\")(end_logits)\n",
        "\n",
        "\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs = [input_ids, attention_mask, attention_spectrum],\n",
        "        outputs= [start_probs, end_probs],\n",
        "    )\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "\n",
        "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_model(num_spectrum_formulas = 2)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPhf6TDt9gnQ"
      },
      "outputs": [],
      "source": [
        "len_dataset = len(dataset_dict_x['input_ids'])\n",
        "size_split = round(len_dataset * TEST_SIZE)\n",
        "num_split = round(len_dataset / size_split)\n",
        "\n",
        "\n",
        "NEW_TOKENIZER = transformers.AutoTokenizer.from_pretrained(\"mahdifar/codeflaws-tokenizer\")\n",
        "\n",
        "\n",
        "def spit_dct_to_train_test(dct, rng_slct):\n",
        "    train_dct, test_dct = {}, {}\n",
        "    for key in dct:\n",
        "        train_dct[key] = [lst for idx, lst in enumerate(dct[key]) if not idx in rng_slct]\n",
        "        test_dct[key]  = [lst for idx, lst in enumerate(dct[key]) if idx in rng_slct]\n",
        "    return train_dct, test_dct\n",
        "\n",
        "\n",
        "\n",
        "def chk_top_k_prd_model(model, dataset, spectrum_prediction, lst_compare, tokenizer):\n",
        "\n",
        "    # Store in the np.array() to use with np.cumsum() & find line_num for each tokens\n",
        "    arr_input_ids = np.asarray(list(dataset.unbatch().map(lambda x, y: x['input_ids'])))\n",
        "\n",
        "    # Store in the np.array() to create range start_end\n",
        "    true_arr_start_token_idxs = np.asarray(list(dataset.unbatch().map(lambda x, y: y['start_token_idx'])))\n",
        "    true_arr_end_token_idxs = np.asarray(list(dataset.unbatch().map(lambda x, y: y['end_token_idx'])))\n",
        "\n",
        "    # create a range from start to end for true answer\n",
        "    true_arr_start_end_token_idxs = np.hstack([true_arr_start_token_idxs[:, None], true_arr_end_token_idxs[:, None]])\n",
        "\n",
        "    for idx_1, rng_true in enumerate(true_arr_start_end_token_idxs):\n",
        "        # Handling to insert last element to range\n",
        "        rng_true[-1] += 1\n",
        "        tmp_rng_true = np.arange(*rng_true)\n",
        "        end_line_token_id = tokenizer.encode('\\n')[1]\n",
        "        # Find each token of input_ids exists in which line\n",
        "        arr_lines_input_ids = (arr_input_ids[idx_1] == end_line_token_id).cumsum()\n",
        "        cnt_true_len_line = np.unique(arr_lines_input_ids, return_counts=True)\n",
        "        dct_cnt_true_len_line = dict(zip(*cnt_true_len_line))\n",
        "        true_lines_error = arr_lines_input_ids[tmp_rng_true]\n",
        "        true_line_error = collections.Counter(true_lines_error).most_common(1)[0][0]\n",
        "        dct_tmp_store_result = {}\n",
        "\n",
        "        for model_name, model_predict in models.items():\n",
        "            # Get the prediction of the model on the test_dataset\n",
        "            pred_arr_start_token_idxs, pred_arr_end_token_idxs = model_predict\n",
        "\n",
        "            # Index of sorting predict array\n",
        "            pred_arr_start_token_idxs = np.argsort(-pred_arr_start_token_idxs, axis=1)\n",
        "            pred_arr_end_token_idxs = np.argsort(-pred_arr_end_token_idxs, axis=1)\n",
        "\n",
        "            # create a range from start to end for prediction answer\n",
        "            pred_arr_start_end_token_idxs = np.dstack([pred_arr_start_token_idxs[..., None], pred_arr_end_token_idxs[..., None]])\n",
        "\n",
        "            lst_dct_store_pred = []\n",
        "            for idx_2, rng_pred in enumerate(pred_arr_start_end_token_idxs[idx_1]):\n",
        "                # Handling to insert last element to range\n",
        "                rng_pred[-1] += 1\n",
        "                tmp_rng_pred = np.arange(*rng_pred)\n",
        "                pred_lines_error = arr_lines_input_ids[tmp_rng_pred]\n",
        "                cnt_pred_len_line = np.unique(pred_lines_error, return_counts=True)\n",
        "                dct_cnt_pred_len_line = dict(zip(*cnt_pred_len_line))\n",
        "                lst_dct_store_pred.append(dct_cnt_pred_len_line)\n",
        "\n",
        "\n",
        "            num_pred_model_prediction = 0\n",
        "            for dct_store_pred in lst_dct_store_pred:\n",
        "                dct_pred_div_true = {k: v/dct_cnt_true_len_line[k] for k,v in dct_store_pred.items()}\n",
        "                sum_dct_pred_div_true_vals = sum(dct_pred_div_true.values())\n",
        "                len_true_prediction_base_tokens = dct_pred_div_true.get(true_line_error, 0)\n",
        "                if len_true_prediction_base_tokens > 0.8:\n",
        "                    num_pred_model_prediction += (sum_dct_pred_div_true_vals / len(dct_pred_div_true.values()))\n",
        "                    break\n",
        "                else:\n",
        "                    if sum_dct_pred_div_true_vals < 1:\n",
        "                        sum_dct_pred_div_true_vals = 1\n",
        "                    num_pred_model_prediction += sum_dct_pred_div_true_vals\n",
        "            dct_tmp_store_result[model_name] = num_pred_model_prediction\n",
        "\n",
        "\n",
        "        dct_weight_error_suspicious = spectrum_prediction['dct_weight_error_suspicious'][idx_1]\n",
        "        arr_line_num_error = np.array(spectrum_prediction['lst_line_num_error'][idx_1])\n",
        "        for formula in dct_weight_error_suspicious:\n",
        "            # If code dose not have any error\n",
        "            if (\n",
        "                (arr_line_num_error == [0]) or\n",
        "                (dct_weight_error_suspicious[formula] == {}) or\n",
        "                (arr_line_num_error[0] not in dct_weight_error_suspicious[formula])):\n",
        "                # Set maximum line_number instead of \"np.nan\" (because we need to check all lines)\n",
        "                dct_tmp_store_result[formula] = arr_lines_input_ids.max()\n",
        "            else:\n",
        "                rank_lines = pd.Series(dct_weight_error_suspicious[formula]).rank(method='average', ascending=False)\n",
        "                line_num_error = arr_line_num_error[0]\n",
        "                dct_tmp_store_result[formula] = rank_lines[line_num_error]\n",
        "        lst_compare.append(dct_tmp_store_result)\n",
        "\n",
        "\n",
        "\n",
        "def create_only_nlp_model():\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN,), name = 'input_ids', dtype=tf.int32)\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN,), name = 'attention_mask', dtype=tf.int32)\n",
        "    attention_spectrum = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN, 2), name = 'attention_spectrum', dtype=tf.float32)\n",
        "\n",
        "    CODEBERT_BASE = transformers.TFRobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "    output_codebert_base = CODEBERT_BASE([input_ids, attention_mask]).last_hidden_state\n",
        "\n",
        "\n",
        "    output_concat = tf.keras.layers.Dense(1024, use_bias=False)(output_codebert_base)\n",
        "    output_concat = tf.keras.layers.Dense(512, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(64, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(16, use_bias=False)(output_concat)\n",
        "\n",
        "    start_logits = tf.keras.layers.Dense(1, use_bias=False)(output_concat)\n",
        "    start_logits = tf.keras.layers.Flatten()(start_logits)\n",
        "    start_probs  = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"start_token_idx\")(start_logits)\n",
        "\n",
        "    end_logits = tf.keras.layers.Dense(1, use_bias=False)(output_concat)\n",
        "    end_logits = tf.keras.layers.Flatten()(end_logits)\n",
        "    end_probs  = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"end_token_idx\")(end_logits)\n",
        "\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs = [input_ids, attention_mask, attention_spectrum],\n",
        "        outputs= [start_probs, end_probs],\n",
        "    )\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_nlp_plus_spectrum_model(num_spectrum_formulas = 2):\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN,), name = 'input_ids', dtype=tf.int32)\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN,), name = 'attention_mask', dtype=tf.int32)\n",
        "    attention_spectrum = tf.keras.layers.Input(shape=(MAX_LEN_TOKEN, num_spectrum_formulas), name = 'attention_spectrum', dtype=tf.float32)\n",
        "\n",
        "    CODEBERT_BASE = transformers.TFRobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "    output_codebert_base = CODEBERT_BASE([input_ids, attention_mask]).last_hidden_state\n",
        "    output_concat = tf.keras.layers.Lambda(lambda x: tf.concat([x[0], x[1]], -1))([output_codebert_base, attention_spectrum])\n",
        "\n",
        "    output_concat = tf.keras.layers.Dense(1024, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(512, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(64, use_bias=False)(output_concat)\n",
        "    output_concat = tf.keras.layers.Dense(16, use_bias=False)(output_concat)\n",
        "\n",
        "\n",
        "    start_logits = tf.keras.layers.Dense(1, use_bias=False)(output_concat)\n",
        "    start_logits = tf.keras.layers.Flatten()(start_logits)\n",
        "    start_probs  = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"start_token_idx\")(start_logits)\n",
        "\n",
        "\n",
        "    end_logits = tf.keras.layers.Dense(1, use_bias=False)(output_concat)\n",
        "    end_logits = tf.keras.layers.Flatten()(end_logits)\n",
        "    end_probs  = tf.keras.layers.Activation(tf.keras.activations.softmax, name=\"end_token_idx\")(end_logits)\n",
        "\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs = [input_ids, attention_mask, attention_spectrum],\n",
        "        outputs= [start_probs, end_probs],\n",
        "    )\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "lst_compare_different_dataset = []\n",
        "for i_ns in range(num_split):\n",
        "\n",
        "    # Select range for split dataset to train & test\n",
        "    rng_slct = range(i_ns*size_split, (i_ns+1)*size_split)\n",
        "\n",
        "\n",
        "    # Split dict_dataset to train and test base index of range_select\n",
        "    train_dataset_dict_x, test_dataset_dict_x = spit_dct_to_train_test(dataset_dict_x, rng_slct)\n",
        "    train_dataset_dict_y, test_dataset_dict_y = spit_dct_to_train_test(dataset_dict_y, rng_slct)\n",
        "\n",
        "\n",
        "    # Get test_split of dataset_spectrum to compare with predict of model\n",
        "    _, test_dataset_spectrum = spit_dct_to_train_test(dataset_spectrum, rng_slct)\n",
        "\n",
        "\n",
        "    # Creating tf.data.Dataset of train & test\n",
        "    train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_dataset_dict_x, train_dataset_dict_y)).batch(BATCH_SIZE)\n",
        "    test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_dataset_dict_x, test_dataset_dict_y)).batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "    nlp_plus_spectrum_model = create_nlp_plus_spectrum_model(num_spectrum_formulas=2)\n",
        "    # Train the model with train_tf_dataset\n",
        "    nlp_plus_spectrum_model.fit(train_tf_dataset, epochs=4, validation_data=test_tf_dataset)\n",
        "    # Get the prediction of the model on the test_dataset\n",
        "    nlp_plus_spectrum_model_predict = nlp_plus_spectrum_model.predict(test_tf_dataset)\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    del nlp_plus_spectrum_model\n",
        "\n",
        "\n",
        "    only_nlp_model = create_only_nlp_model()\n",
        "    # Train the model with train_tf_dataset\n",
        "    only_nlp_model.fit(train_tf_dataset, epochs=4, validation_data=test_tf_dataset)\n",
        "    # Get the prediction of the model on the test_dataset\n",
        "    only_nlp_model_predict = only_nlp_model.predict(test_tf_dataset)\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    del only_nlp_model\n",
        "\n",
        "\n",
        "    models = {\n",
        "        'only_nlp_model'  : only_nlp_model_predict,\n",
        "        'nlp_plus_spectrum_model' : nlp_plus_spectrum_model_predict,\n",
        "    }\n",
        "\n",
        "    # Predict The result by test_tf_dataset and find top_k\n",
        "    chk_top_k_prd_model(models, dataset = test_tf_dataset,\n",
        "                        spectrum_prediction = test_dataset_spectrum,\n",
        "                        lst_compare = lst_compare_different_dataset,\n",
        "                        tokenizer = NEW_TOKENIZER)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
